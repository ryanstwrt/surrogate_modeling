{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial hyper-parameter Sensitivity Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will investigate the various sensitivty models and how a set of hyper-parameters effect the accuracy of the surrogate model.\n",
    "The surrogate model is based off of a sodium fast reactor, whose design space ranges includes fuel height, smear, and plutonium content.\n",
    "The objectives that the surrogate model is predicting are k-eff, void coefficent, and Doppler coefficient.\n",
    "There are 60 datapoints in the database that the surrogate models can use to train on.\n",
    "\n",
    "This section examines how various hyper-parameters effect the accuracy of the surrogate model, and examines a validation curve for each of the parameters.\n",
    "The models will examine each objective independently, i.e. utilize the design inputs to map to only one objective, and as a whole.\n",
    "Separating the objective will help differentiate how the hyper-parameters effects predicitions for each objective.\n",
    "\n",
    "The cross-validation score is used to measure how the surrogate models fits the data to the model.\n",
    "Typically, when both the training and cross-validation scores are low, it mean the surrogate model is unfitting.\n",
    "If the training score is hight, but the cross-validation score is low, the surrogate model is overfitting the data, and is not able to accurately predict new values.\n",
    "The ideal is to have the training and cross-validation score to be high, such that we are accurately predicting the objectives.\n",
    "In the graphs presented, the dark line is the mean score, which is surrounded by the standard deviation.\n",
    "\n",
    "The hyper parameters will be given in the form: [min, max, # Iterations].\n",
    "Each model has the random variable set to 0, to prevent results from changing between each examination.\n",
    "It is noted that linear regression does not have any hyper-parameters, and is not listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_surrogate_models as tm\n",
    "import db_reshape as dbr\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "var_tot, obj_tot = dbr.reshape_database(r'sfr_db.h5', ['height', 'smear', 'pu_content'], ['keff', 'void_coeff', 'doppler_coeff'])\n",
    "var_tot, obj_keff = dbr.reshape_database(r'sfr_db.h5', ['height', 'smear', 'pu_content'], ['keff'])\n",
    "var_tot, obj_void = dbr.reshape_database(r'sfr_db.h5', ['height', 'smear', 'pu_content'], ['void_coeff'])\n",
    "var_tot, obj_dopp = dbr.reshape_database(r'sfr_db.h5', ['height', 'smear', 'pu_content'], ['doppler_coeff'])\n",
    "\n",
    "\n",
    "data_col = ['model', 'r-squared', 'MSE', 'mean', 'std', 'hyper-parameters', 'cv_results']\n",
    "sm_db = pd.DataFrame(columns = data_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "Hyper-parameter: *Poly-Degree* - [1, 7, 1]\n",
    "\n",
    "Results:\n",
    "\n",
    "We note a few major aspect of the graphs to examine.\n",
    "Polynomials of order greater than 4 provide dreadful results, which helps indicate that the underlying physics likely does not require polynomials of this degree.\n",
    "Along with this, both k-eff and the void coefficient have a high cross-validation score for degree 2 and 3, whereas the doppler coefficient sees decrease in over 50% between these values.\n",
    "\n",
    "This plummet carries over when all of the objective are examined in tandem.\n",
    "For a polynomial of degree 2, the cross-validation score is ~0.95 and decreases to < 0.8 for a 3rd order polynomial.\n",
    "This is likely due to the failure of the model to capture the Doppler coefficient.\n",
    "\n",
    "For this particular database, it is suggested that between 1st and 3th order polynomials are examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = tm.Surrogate_Models()\n",
    "model = 'pr'\n",
    "name = ['keff', 'void', 'dopp', 'tot']\n",
    "objective = [obj_keff, obj_void, obj_dopp, obj_tot]\n",
    "\n",
    "for name, obj in zip(name, objective):\n",
    "    print(name)\n",
    "    sm.random = 0\n",
    "    sm.update_database(var_tot, obj)\n",
    "    sm.update_model(model)\n",
    "    sm.plot_validation_curve(model, 'poly__degree', np.linspace(1,7,7,dtype=np.int16))\n",
    "    sm.clear_surrogate_model()\n",
    "    \n",
    "sm.update_database(var_tot, obj_tot)\n",
    "sm.update_model(model)\n",
    "sm.optimize_model(model)\n",
    "score = sm.models[model]['score']\n",
    "mse_score = sm.models[model]['mse_score']\n",
    "hp = sm.models[model]['hyper_parameters']\n",
    "cv = sm.models[model]['cv_results']\n",
    "append_dict = pd.DataFrame([[model,\n",
    "                             score,\n",
    "                             mse_score,\n",
    "                             sm_db['r-squared'].mean(axis = 0),\n",
    "                             sm_db['r-squared'].std(axis = 0),\n",
    "                             hp,\n",
    "                             cv]], columns=data_col)\n",
    "sm_db = sm_db.append(append_dict)\n",
    "print('Poly Degree: {}; Score {}'.format(hp, score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Adaptive Regression Splines\n",
    "\n",
    "Hyper-parameter: *endspan_alpha* - [0.01, 0.1, 100] <br />\n",
    "                 *minspan_alpha* - [0.01, 0.1, 100] <br />\n",
    "                 *penalty*       - [1, 10, 10] <br />\n",
    "                 \n",
    "Results:\n",
    "\n",
    "The resulst for using MARS shows the ability to remove multiple hyper-parameters due to a relative insensitivity.\n",
    "For *endspan_alpha*, there is little variation in cross-validation score for k-eff and the void coefficient.\n",
    "For both the Doppler and the combined objectives, there is an increase of ~10% when varying the *endspan_alhpa*.\n",
    "Given the relative invariance for k-eff and the void coeffificient, the value for *endspan_alpha* should be set close to 0.1 to take advantage of this.\n",
    "It should be noted that additional cross-validation scores should be examined to ensure that increasing *endspan_alpha* does not negatively effect any of the other parameters.\n",
    "\n",
    "The results for *minspan_alpha* indicate that none of the objective show any sensitivity to the values, and thus the default value should be used.\n",
    "\n",
    "For the *penalty*, all of the objective show a similar trend.\n",
    "There is considerable variation in the cross-validation score between 0 and 6, where after this it is relatively constant and produces a high cross-validation score.\n",
    "Given the common trends among the objectives, it is recommended that the *penalty* be set to a value greater than 6.\n",
    "\n",
    "For this dataset, an *endspan_alpha* of ~0.1 should be used with a penalty of >6, where the *minspan_alpha* shoudl be let as the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = tm.Surrogate_Models()\n",
    "model = 'mars'\n",
    "\n",
    "name = ['keff', 'void', 'dopp', 'tot']\n",
    "objective = [obj_keff, obj_void, obj_dopp, obj_tot]\n",
    "\n",
    "for name, obj in zip(name, objective):\n",
    "    print(name)\n",
    "    sm.random = 0\n",
    "    sm.update_database(var_tot, obj)\n",
    "    sm.update_model(model)\n",
    "    sm.plot_validation_curve(model, 'endspan_alpha', np.linspace(0.01,0.1,100))\n",
    "    sm.plot_validation_curve(model, 'minspan_alpha', np.linspace(0.01,0.1,100))\n",
    "    sm.plot_validation_curve(model, 'penalty', np.linspace(1,10,10))\n",
    "    print('')\n",
    "    sm.clear_surrogate_model()\n",
    "\n",
    "\n",
    "sm.update_database(var_tot, obj_tot)\n",
    "sm.update_model(model)\n",
    "sm.optimize_model(model)\n",
    "score = sm.models[model]['score']\n",
    "mse_score = sm.models[model]['mse_score']\n",
    "hp = sm.models[model]['hyper_parameters']\n",
    "cv = sm.models[model]['cv_results']\n",
    "append_dict = pd.DataFrame([[model,\n",
    "                             score,\n",
    "                             mse_score,\n",
    "                             sm_db['r-squared'].mean(axis = 0),\n",
    "                             sm_db['r-squared'].std(axis = 0),\n",
    "                             hp,\n",
    "                             cv]], columns=data_col)\n",
    "sm_db = sm_db.append(append_dict)\n",
    "print('MARS Degree: {}; Score {}'.format(hp, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Regression\n",
    "\n",
    "Hyper-parameter: alpha - [1e-11, 1e-9, 100]\n",
    "\n",
    "Results:\n",
    "\n",
    "For the Gaussian Process Regression, the only parameter examined here was *alpha*.\n",
    "It was found that the cross-validation error is virtually invariant with respect to this parameter.\n",
    "As such, the value for *alpha* should be left as the default value.\n",
    "\n",
    "Notes:\n",
    "\n",
    "There is another parameter to examine for GPR, namely the kernel used to evaluate the function.\n",
    "Three kernels (*RBF*, *Matern*, *RotationalQuadratic*) are built into *train_surrogate_models*, however additional work is needed to be able to adjust the hyper-parameters of these kernels to examine the validation curve.\n",
    "When this has been built in, additional results will be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import Matern, RBF, RationalQuadratic\n",
    "\n",
    "sm = tm.Surrogate_Models()\n",
    "model = 'gpr'\n",
    "\n",
    "name = ['keff', 'void', 'dopp', 'tot']\n",
    "objective = [obj_keff, obj_void, obj_dopp, obj_tot]\n",
    "\n",
    "for name, obj in zip(name, objective):\n",
    "    print(name)\n",
    "    sm.random = 0\n",
    "    sm.update_database(var_tot, obj)\n",
    "    sm.update_model(model)\n",
    "    sm.plot_validation_curve(model, 'alpha', np.linspace(1e-11,1e-9,100))\n",
    "    print('')\n",
    "    sm.clear_surrogate_model()\n",
    "\n",
    "\n",
    "sm.update_database(var_tot, obj_tot)\n",
    "sm.update_model(model)\n",
    "sm.optimize_model(model)\n",
    "score = sm.models[model]['score']\n",
    "mse_score = sm.models[model]['mse_score']\n",
    "hp = sm.models[model]['hyper_parameters']\n",
    "cv = sm.models[model]['cv_results']\n",
    "append_dict = pd.DataFrame([[model,\n",
    "                             score,\n",
    "                             mse_score,\n",
    "                             sm_db['r-squared'].mean(axis = 0),\n",
    "                             sm_db['r-squared'].std(axis = 0),\n",
    "                             hp,\n",
    "                             cv]], columns=data_col)\n",
    "sm_db = sm_db.append(append_dict)\n",
    "print('Poly Degree: {}; Score {}'.format(hp, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network\n",
    "\n",
    "Hyper-parameter: *hidden_layer_size* - [1,50,50] <br />\n",
    "                 *alpha*             - [0.00001,0.001,100]\n",
    "\n",
    "Results:\n",
    "\n",
    "Looking first at the results for *hidden_layer_size*, the trend for k-eff and the void coefficient is an increase in the cross-validaton score during the initial 1-20 hidden layers.\n",
    "After this, the increase in the cross-validation score stagnates.\n",
    "For the Doppler and total objectives, there is relatively little difference in the CV score due to the the number of hidden layers.\n",
    "Overall, a value between 10 and 25 seems reasonable to capture the benefits of additional hidden layers.\n",
    "\n",
    "The value of *alpha* is relatively noise and tends to hover around a constant value. As such, the recommended value is likely acceptable.\n",
    "\n",
    "For this dataset, the *hidden_layer_size* shoudl be examined between 10 and 15, while the *alpha* can remain at its default value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = tm.Surrogate_Models()\n",
    "model = 'ann'\n",
    "\n",
    "name = ['keff', 'void', 'dopp', 'tot']\n",
    "objective = [obj_keff, obj_void, obj_dopp, obj_tot]\n",
    "\n",
    "for name, obj in zip(name, objective):\n",
    "    print(name)\n",
    "    sm.random = 0 \n",
    "    sm.update_database(var_tot, obj)\n",
    "    sm.update_model(model)\n",
    "    sm.plot_validation_curve(model, 'hidden_layer_sizes', np.linspace(1,50,50,dtype=np.int16))\n",
    "    sm.plot_validation_curve(model, 'alpha', np.linspace(0.00001,0.001,100))\n",
    "    print('')\n",
    "    sm.clear_surrogate_model()\n",
    "\n",
    "\n",
    "sm.update_database(var_tot, obj_tot)\n",
    "sm.update_model(model)\n",
    "sm.optimize_model(model)\n",
    "score = sm.models[model]['score']\n",
    "mse_score = sm.models[model]['mse_score']\n",
    "hp = sm.models[model]['hyper_parameters']\n",
    "cv = sm.models[model]['cv_results']\n",
    "append_dict = pd.DataFrame([[model,\n",
    "                             score,\n",
    "                             mse_score,\n",
    "                             sm_db['r-squared'].mean(axis = 0),\n",
    "                             sm_db['r-squared'].std(axis = 0),\n",
    "                             hp,\n",
    "                             cv]], columns=data_col)\n",
    "sm_db = sm_db.append(append_dict)\n",
    "print('ANN: {}; Score {}'.format(hp, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Hyper-parameters: *n-estimators* - [10,500,490] <br />\n",
    "*min_samples_leaf* - [1,10,10] <br />\n",
    "*min_samples_split* - [1,10,10] <br />\n",
    "\n",
    "Results:\n",
    "\n",
    "For the *min_samples_leaf* and *min_samples_split*, increasing the minimum number of samples required to be considered at a leaf node or an internal node causes a significant decrease the the CV score.\n",
    "For nearly all of the objectives, at a minimum of a 50% reduction in the CV score.\n",
    "This indicates a high importance should be placed on the value for both *min_samples_leaf* and *min_samples_split*, where a values of 1 should be used for each to minimize the reductin in the CV score.\n",
    "\n",
    "The *n-estimators* has a lower sensitivity compared to the previous hyper-parameters. \n",
    "Using a small number of estimators (<100) tend to produce slightly worse results on average, than thos using >100.\n",
    "Despite this, the difference between 100 and 500, shows little improvement in the CV score, and thus a value in this range would seem appropriate.\n",
    "\n",
    "For this data set, the *min_samples_leaf* and *min_samples_split* should be set to 1, while the *n-estimators* should be set between 100 and 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = tm.Surrogate_Models()\n",
    "model = 'rf'\n",
    "\n",
    "name = ['keff', 'void', 'dopp', 'tot']\n",
    "objective = [obj_keff, obj_void, obj_dopp, obj_tot]\n",
    "\n",
    "for name, obj in zip(name, objective):\n",
    "    print(name)\n",
    "    sm.random = 0\n",
    "    sm.update_database(var_tot, obj)\n",
    "    sm.update_model(model)\n",
    "    sm.plot_validation_curve(model, 'n_estimators', np.linspace(10,500,250,dtype=np.int16))\n",
    "    sm.plot_validation_curve(model, 'min_samples_leaf', np.linspace(1,10,10,dtype=np.int16))\n",
    "    sm.plot_validation_curve(model, 'min_samples_split', np.linspace(1,10,10,dtype=np.int16))\n",
    "\n",
    "    print('')\n",
    "    sm.clear_surrogate_model()\n",
    "\n",
    "\n",
    "sm.update_database(var_tot, obj_tot)\n",
    "sm.update_model(model)\n",
    "sm.optimize_model(model)\n",
    "score = sm.models[model]['score']\n",
    "mse_score = sm.models[model]['mse_score']\n",
    "hp = sm.models[model]['hyper_parameters']\n",
    "cv = sm.models[model]['cv_results']\n",
    "append_dict = pd.DataFrame([[model,\n",
    "                             score,\n",
    "                             mse_score,\n",
    "                             sm_db['r-squared'].mean(axis = 0),\n",
    "                             sm_db['r-squared'].std(axis = 0),\n",
    "                             hp,\n",
    "                             cv]], columns=data_col)\n",
    "sm_db = sm_db.append(append_dict)\n",
    "print('RM: {}; Score {}'.format(hp, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Squared/MSE\n",
    "\n",
    "The last thing we examine is the R-squared and Mean-Squared Error (MSE) values for each of the 'optimized' models.\n",
    "\n",
    "R-squared is a measure of the proportion of variance in the dependent variables that is predictable by the independent variables.\n",
    "Namely, this describes how well our model predicts the objectives, where a value of 1.0 indicates an exact match between the surrogate model data points.\n",
    "This can give us an indication of the accuracy we can expect when using the surrogate model.\n",
    "\n",
    "MSE measures the average of the squares of the errors between the expected value and the actual value.\n",
    "This describes the quality of the model to predict objectives.\n",
    "For MSE, a value of 0.0 would represent the expected values always aligning with the actual values.\n",
    "Any deviate from this will cause the MSE to increase, and as such high values for the MSE are discouraged.\n",
    "\n",
    "Examining the dataframe above, the random forest estimator stands out as the surrogate model with the highest R-squared and lowest MSE indicating it as the best estimator.\n",
    "It should be noted that this is based on the fact that we forced a random number (0) for all models, and only examined the total objectives rather than looking at each objective individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Conclusions\n",
    "\n",
    "The previous section examined the effect of various hyper-parameters for the small database consisting of SFR reactor parameters.\n",
    "For this research, it was determined that many of the hyper-parameters were unimportant, and do not require further tuning for this particular dataset.\n",
    "However, this does not mean that these hyper-parameters of unimportant for all small dataset.\n",
    "It simply means that relating the fuel height, smear, and plutonium fraction to k-eff, the void coefficient, and Doppler coefficient does not require these parameters.\n",
    "\n",
    "\n",
    "Again, it is noted that the best surrogate model is highly dependent on the data set provided, and the underlying shape which describes amny of the objectives.\n",
    "These guidelines may prove useful for others looking to examine similar design variables and objectives for SFRs, but would likely not be acceptable if the design space was changed to light water reactors.\n",
    "If that were the case, a new surrogate model optimization would need to be performed to ensure adequate models were generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.99396089 -125.71718995   -0.76852932]]\n",
      "[[   1.00610884 -120.15792878   -0.76526771]]\n",
      "[[   0.97672342 -134.99686615   -0.7582642 ]]\n",
      "val. score: 0.9427500155653709\n",
      "mse. score: 0.057249984434629086\n",
      "train score: 0.956362285494724\n",
      "\n",
      "val. score: 0.9427500155653709\n",
      "mse. score: 0.057249984434629086\n",
      "train score: 0.956362285494724\n",
      "None\n",
      "[[   0.99396089 -125.71718995   -0.76852932]]\n",
      "[[   1.00610884 -120.15792878   -0.76526771]]\n",
      "[[   0.97672342 -134.99686615   -0.7582642 ]]\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "import train_surrogate_models as tm\n",
    "import db_reshape as dbr\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "var_tot, obj_tot = dbr.reshape_database(r'sfr_db.h5', ['height', 'smear', 'pu_content'], ['keff', 'void_coeff', 'doppler_coeff'])\n",
    "var_tot, obj_keff = dbr.reshape_database(r'sfr_db.h5', ['height', 'smear', 'pu_content'], ['keff'])\n",
    "var_tot, obj_void = dbr.reshape_database(r'sfr_db.h5', ['height', 'smear', 'pu_content'], ['void_coeff'])\n",
    "var_tot, obj_dopp = dbr.reshape_database(r'sfr_db.h5', ['height', 'smear', 'pu_content'], ['doppler_coeff'])\n",
    "\n",
    "\n",
    "data_col = ['model', 'r-squared', 'MSE', 'mean', 'std', 'hyper-parameters', 'cv_results']\n",
    "sm_db = pd.DataFrame(columns = data_col)\n",
    "\n",
    "\n",
    "sm = tm.Surrogate_Models()\n",
    "model = 'gpr'\n",
    "\n",
    "name = ['keff', 'void', 'dopp', 'tot']\n",
    "objective = [obj_keff, obj_void, obj_dopp, obj_tot]\n",
    "sm.random = 42\n",
    "sm.update_database(var_tot, obj_tot)\n",
    "sm.update_model(model)\n",
    "print(sm.predict(model, [[61.37, 51.58, 0.7374]]))\n",
    "print(sm.predict(model, [[59.72, 50.01, 0.8694]]))\n",
    "print(sm.predict(model, [[71.06, 55.77, 0.3536]]))\n",
    "print(\"val. score: {}\".format(sm.models[model]['score']))\n",
    "print(\"mse. score: {}\".format(sm.models[model]['mse_score']))\n",
    "print(\"train score: {}\".format(sm.models[model]['model'].score(sm.scaled_var_train, sm.scaled_obj_train)))\n",
    "print('')\n",
    "sm.optimize_model(model)\n",
    "score = sm.models[model]['score']\n",
    "mse_score = sm.models[model]['mse_score']\n",
    "\n",
    "print(\"val. score: {}\".format(sm.models[model]['score']))\n",
    "print(\"mse. score: {}\".format(sm.models[model]['mse_score']))\n",
    "print(\"train score: {}\".format(sm.models[model]['model'].score(sm.scaled_var_train, sm.scaled_obj_train)))\n",
    "print(sm.models[model]['hyper_parameters'])\n",
    "print(sm.predict(model, [[61.37, 51.58, 0.7374]]))\n",
    "print(sm.predict(model, [[59.72, 50.01, 0.8694]]))\n",
    "print(sm.predict(model, [[71.06, 55.77, 0.3536]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN:\n",
    "[[   1.00171718 -111.51224791   -0.71113269]]\n",
    "[[   0.99975803 -110.16234341   -0.7367976 ]]\n",
    "[[   0.99597611 -118.09214806   -0.67915484]]\n",
    "val. score: 0.9686457192215108\n",
    "mse. score: 0.031354280778489396\n",
    "train score: 0.9978758650554262\n",
    "\n",
    "val. score: 0.9659870735689806\n",
    "mse. score: 0.03401292643101947\n",
    "train score: 0.9979936174359005\n",
    "{'activation': 'logistic', 'hidden_layer_sizes': 81, 'solver': 'lbfgs'}\n",
    "[[   1.00156397 -112.75257061   -0.7135622 ]]\n",
    "[[   0.9992289  -110.67329216   -0.73885222]]\n",
    "[[   1.00009883 -119.01450595   -0.67470041]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
